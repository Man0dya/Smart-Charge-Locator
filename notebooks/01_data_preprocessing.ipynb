{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75746b4c",
   "metadata": {},
   "source": [
    "# EV charger optimal placement: data prep, modeling, optimization, and a Python Streamlit frontend\n",
    "\n",
    "This notebook walks through the full workflow: data ingestion, cleaning, feature engineering, modeling (regression + classification), clustering- and ILP-based placement optimization, and exporting artifacts for a Streamlit frontend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed2c12",
   "metadata": {},
   "source": [
    "## 1) Project To-Do Checklist (tracked in JSON)\n",
    "We'll maintain a simple tasks.json to track progress across key milestones.\n",
    "- data prep\n",
    "- modeling\n",
    "- placement (KMeans/ILP)\n",
    "- app (Streamlit)\n",
    "\n",
    "The next cell creates/loads `tasks.json` and helper functions to toggle tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b223d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app': False,\n",
      " 'data_prep': False,\n",
      " 'modeling': False,\n",
      " 'nearest_search': False,\n",
      " 'placement': False,\n",
      " 'updated_at': None}\n"
     ]
    }
   ],
   "source": [
    "# Summary: Initialize a simple tasks.json checklist and helper functions; then print current status.\n",
    "import json, os, pprint\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = os.path.abspath(os.path.join(os.path.dirname(os.getcwd())))\n",
    "TASKS_PATH = os.path.join(ROOT, 'tasks.json')\n",
    "\n",
    "def init_tasks():\n",
    "    default = {\n",
    "        'data_prep': False,\n",
    "        'modeling': False,\n",
    "        'placement': False,\n",
    "        'app': False,\n",
    "        'updated_at': None,\n",
    "    }\n",
    "    if not os.path.exists(TASKS_PATH):\n",
    "        with open(TASKS_PATH, 'w') as f:\n",
    "            json.dump(default, f, indent=2)\n",
    "    return default\n",
    "\n",
    "\n",
    "def read_tasks():\n",
    "    if not os.path.exists(TASKS_PATH):\n",
    "        return init_tasks()\n",
    "    with open(TASKS_PATH, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def write_tasks(tasks: dict):\n",
    "    tasks['updated_at'] = datetime.utcnow().isoformat()\n",
    "    with open(TASKS_PATH, 'w') as f:\n",
    "        json.dump(tasks, f, indent=2)\n",
    "\n",
    "\n",
    "def set_task(name: str, value: bool):\n",
    "    tasks = read_tasks()\n",
    "    if name in tasks:\n",
    "        tasks[name] = value\n",
    "        write_tasks(tasks)\n",
    "    else:\n",
    "        print(f'Unknown task: {name}')\n",
    "\n",
    "\n",
    "def show_tasks():\n",
    "    pprint.pprint(read_tasks())\n",
    "\n",
    "init_tasks()\n",
    "show_tasks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89527ee3",
   "metadata": {},
   "source": [
    "## 2) Environment Setup and Project Scaffolding\n",
    "Create folders and write a requirements.txt if missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a921058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaffold ready. You may need to pip install -r requirements.txt in a terminal.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Create project folders and a minimal requirements.txt if missing.\n",
    "import os, textwrap, pathlib\n",
    "\n",
    "folders = [\n",
    "    'data/raw', 'data/processed', 'models', 'artifacts', 'notebooks', 'src', 'app', 'tests'\n",
    "]\n",
    "for d in folders:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "req = pathlib.Path('requirements.txt')\n",
    "if not req.exists():\n",
    "    req.write_text(textwrap.dedent('''\\\n",
    "    pandas>=2.2\n",
    "    numpy>=1.26\n",
    "    scikit-learn>=1.5\n",
    "    scikit-learn-extra>=0.3\n",
    "    scipy>=1.11\n",
    "    xgboost>=2.0\n",
    "    lightgbm>=4.3\n",
    "    imbalanced-learn>=0.12\n",
    "    geopandas>=0.14\n",
    "    shapely>=2.0\n",
    "    folium>=0.15\n",
    "    geopy>=2.4\n",
    "    pulp>=2.8\n",
    "    seaborn>=0.13\n",
    "    matplotlib>=3.8\n",
    "    joblib>=1.4\n",
    "    streamlit>=1.36\n",
    "    pyarrow>=16.0\n",
    "    '''))\n",
    "\n",
    "print('Scaffold ready. You may need to pip install -r requirements.txt in a terminal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97a815",
   "metadata": {},
   "source": [
    "## 3) Data Ingestion (Kaggle/local) and Raw Snapshot\n",
    "We will read the large CSV from repo root if present, else from data/raw, and snapshot to Parquet for faster reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8da0998",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Summary: Read EV CSV in chunks, sample ~1M rows, and persist a fast Parquet snapshot.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\FDM\\PROJECT\\NEW\\Smart-Charge-Locator\\.venv\\Lib\\site-packages\\pandas\\__init__.py:31\u001b[39m\n\u001b[32m     28\u001b[39m         _missing_dependencies.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(_missing_dependencies)\n\u001b[32m     33\u001b[39m     )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "# Summary: Read EV CSV in chunks, sample ~1M rows, and persist a fast Parquet snapshot.\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve project root (parent of notebooks directory)\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "CSV_CANDIDATES = [\n",
    "    ROOT_DIR / 'Electric_Vehicle_Population_Data.csv',\n",
    "    ROOT_DIR / 'data' / 'raw' / 'Electric_Vehicle_Population_Data.csv'\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for p in CSV_CANDIDATES:\n",
    "    if p.exists():\n",
    "        csv_path = p\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(f'Place Electric_Vehicle_Population_Data.csv at {ROOT_DIR} or {ROOT_DIR / \"data\" / \"raw\"}.')\n",
    "\n",
    "print('Reading CSV (first ~1e6 rows if huge) ...')\n",
    "# Read in chunks to avoid memory limits\n",
    "chunks = pd.read_csv(csv_path, chunksize=200_000, low_memory=False)\n",
    "first = next(chunks)\n",
    "cols = list(first.columns)\n",
    "\n",
    "# Take a few chunks to build a profiling sample (adjust N if needed)\n",
    "sample_list = [first]\n",
    "for i, c in enumerate(chunks, start=1):\n",
    "    sample_list.append(c)\n",
    "    if i >= 4:  # ~1M rows total (5 chunks * 200k)\n",
    "        break\n",
    "\n",
    "df_raw = pd.concat(sample_list, ignore_index=True)\n",
    "print('Raw sample shape:', df_raw.shape)\n",
    "\n",
    "raw_snapshot = ROOT_DIR / 'data' / 'processed' / 'raw_snapshot.parquet'\n",
    "raw_snapshot.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_raw.to_parquet(raw_snapshot, index=False)\n",
    "print('Saved snapshot to', raw_snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c1f71",
   "metadata": {},
   "source": [
    "## 4) Data Profiling and Schema Fixes\n",
    "Peek at columns, types, uniques, and standardize column names to snake_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original -> New column names (first 15):\n",
      "- VIN (1-10) -> vin_1_10\n",
      "- County -> county\n",
      "- City -> city\n",
      "- State -> state\n",
      "- Postal Code -> postal_code\n",
      "- Model Year -> model_year\n",
      "- Make -> make\n",
      "- Model -> model\n",
      "- Electric Vehicle Type -> electric_vehicle_type\n",
      "- Clean Alternative Fuel Vehicle (CAFV) Eligibility -> clean_alternative_fuel_vehicle_cafv_eligibility\n",
      "- Electric Range -> electric_range\n",
      "- Base MSRP -> base_msrp\n",
      "- Legislative District -> legislative_district\n",
      "- DOL Vehicle ID -> dol_vehicle_id\n",
      "- Vehicle Location -> vehicle_location\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250659 entries, 0 to 250658\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                           Non-Null Count   Dtype  \n",
      "---  ------                                           --------------   -----  \n",
      " 0   vin_1_10                                         250659 non-null  object \n",
      " 1   county                                           250653 non-null  object \n",
      " 2   city                                             250653 non-null  object \n",
      " 3   state                                            250659 non-null  object \n",
      " 4   postal_code                                      250653 non-null  float64\n",
      " 5   model_year                                       250659 non-null  int64  \n",
      " 6   make                                             250659 non-null  object \n",
      " 7   model                                            250659 non-null  object \n",
      " 8   electric_vehicle_type                            250659 non-null  object \n",
      " 9   clean_alternative_fuel_vehicle_cafv_eligibility  250659 non-null  object \n",
      " 10  electric_range                                   250638 non-null  float64\n",
      " 11  base_msrp                                        250638 non-null  float64\n",
      " 12  legislative_district                             250076 non-null  float64\n",
      " 13  dol_vehicle_id                                   250659 non-null  int64  \n",
      " 14  vehicle_location                                 250645 non-null  object \n",
      " 15  electric_utility                                 250653 non-null  object \n",
      " 16  2020_census_tract                                250653 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(10)\n",
      "memory usage: 32.5+ MB\n",
      "None\n",
      "\n",
      "Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vin_1_10</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>model_year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>electric_vehicle_type</th>\n",
       "      <th>clean_alternative_fuel_vehicle_cafv_eligibility</th>\n",
       "      <th>electric_range</th>\n",
       "      <th>base_msrp</th>\n",
       "      <th>legislative_district</th>\n",
       "      <th>dol_vehicle_id</th>\n",
       "      <th>vehicle_location</th>\n",
       "      <th>electric_utility</th>\n",
       "      <th>2020_census_tract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5YJSA1E65N</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Granger</td>\n",
       "      <td>WA</td>\n",
       "      <td>98932.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL S</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>Eligibility unknown as battery range has not b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>187279214</td>\n",
       "      <td>POINT (-120.1871 46.33949)</td>\n",
       "      <td>PACIFICORP</td>\n",
       "      <td>5.307700e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNDC3DLC5N</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>WA</td>\n",
       "      <td>98902.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>KIA</td>\n",
       "      <td>EV6</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>Eligibility unknown as battery range has not b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>210098241</td>\n",
       "      <td>POINT (-120.52041 46.59751)</td>\n",
       "      <td>PACIFICORP</td>\n",
       "      <td>5.307700e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5YJYGDEEXL</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Everett</td>\n",
       "      <td>WA</td>\n",
       "      <td>98208.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL Y</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>Clean Alternative Fuel Vehicle Eligible</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>121781950</td>\n",
       "      <td>POINT (-122.18637 47.89251)</td>\n",
       "      <td>PUGET SOUND ENERGY INC</td>\n",
       "      <td>5.306104e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3C3CFFGE1G</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>WA</td>\n",
       "      <td>98908.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>FIAT</td>\n",
       "      <td>500</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>Clean Alternative Fuel Vehicle Eligible</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>180778377</td>\n",
       "      <td>POINT (-120.60199 46.59817)</td>\n",
       "      <td>PACIFICORP</td>\n",
       "      <td>5.307700e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNDCC3LD5K</td>\n",
       "      <td>Kitsap</td>\n",
       "      <td>Bremerton</td>\n",
       "      <td>WA</td>\n",
       "      <td>98312.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>KIA</td>\n",
       "      <td>NIRO</td>\n",
       "      <td>Plug-in Hybrid Electric Vehicle (PHEV)</td>\n",
       "      <td>Not eligible due to low battery range</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2581225</td>\n",
       "      <td>POINT (-122.65223 47.57192)</td>\n",
       "      <td>PUGET SOUND ENERGY INC</td>\n",
       "      <td>5.303508e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     vin_1_10     county       city state  postal_code  model_year   make    model                   electric_vehicle_type    clean_alternative_fuel_vehicle_cafv_eligibility  \\\n",
       "0  5YJSA1E65N     Yakima    Granger    WA      98932.0        2022  TESLA  MODEL S          Battery Electric Vehicle (BEV)  Eligibility unknown as battery range has not b...   \n",
       "1  KNDC3DLC5N     Yakima     Yakima    WA      98902.0        2022    KIA      EV6          Battery Electric Vehicle (BEV)  Eligibility unknown as battery range has not b...   \n",
       "2  5YJYGDEEXL  Snohomish    Everett    WA      98208.0        2020  TESLA  MODEL Y          Battery Electric Vehicle (BEV)            Clean Alternative Fuel Vehicle Eligible   \n",
       "3  3C3CFFGE1G     Yakima     Yakima    WA      98908.0        2016   FIAT      500          Battery Electric Vehicle (BEV)            Clean Alternative Fuel Vehicle Eligible   \n",
       "4  KNDCC3LD5K     Kitsap  Bremerton    WA      98312.0        2019    KIA     NIRO  Plug-in Hybrid Electric Vehicle (PHEV)              Not eligible due to low battery range   \n",
       "\n",
       "   electric_range  base_msrp  legislative_district  dol_vehicle_id             vehicle_location        electric_utility  2020_census_tract  \n",
       "0             0.0        0.0                  15.0       187279214   POINT (-120.1871 46.33949)              PACIFICORP       5.307700e+10  \n",
       "1             0.0        0.0                  15.0       210098241  POINT (-120.52041 46.59751)              PACIFICORP       5.307700e+10  \n",
       "2           291.0        0.0                  44.0       121781950  POINT (-122.18637 47.89251)  PUGET SOUND ENERGY INC       5.306104e+10  \n",
       "3            84.0        0.0                  14.0       180778377  POINT (-120.60199 46.59817)              PACIFICORP       5.307700e+10  \n",
       "4            26.0        0.0                  26.0         2581225  POINT (-122.65223 47.57192)  PUGET SOUND ENERGY INC       5.303508e+10  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: Standardize column names to snake_case, print info and head for quick schema profiling.\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 180)\n",
    "\n",
    "def to_snake(s: str) -> str:\n",
    "    s = re.sub(r'[^0-9A-Za-z]+', '_', s)\n",
    "    s = re.sub(r'_+', '_', s).strip('_')\n",
    "    return s.lower()\n",
    "\n",
    "orig_cols = df_raw.columns.tolist()\n",
    "df = df_raw.copy()\n",
    "df.columns = [to_snake(c) for c in df.columns]\n",
    "\n",
    "print('Original -> New column names (first 15):')\n",
    "for o, n in zip(orig_cols[:15], df.columns[:15]):\n",
    "    print(f'- {o} -> {n}')\n",
    "\n",
    "print('\\nInfo:')\n",
    "print(df.info())\n",
    "\n",
    "print('\\nHead:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d20ac",
   "metadata": {},
   "source": [
    "## 5) Missing Value Handling\n",
    "Impute numeric with median and categoricals with mode ('Unknown'). Persist the imputers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf473918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation complete.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Impute missing numeric (median) and categorical (most frequent) values and persist imputers.\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns]\n",
    "cat_cols = [c for c in df.select_dtypes(exclude=[np.number]).columns]\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "if num_cols:\n",
    "    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "if cat_cols:\n",
    "    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "joblib.dump(num_imputer, 'artifacts/num_imputer.pkl')\n",
    "joblib.dump(cat_imputer, 'artifacts/cat_imputer.pkl')\n",
    "\n",
    "print('Imputation complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3aac1d",
   "metadata": {},
   "source": [
    "## 6) Irrelevant Column Removal\n",
    "Drop identifiers and columns with too many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['vin_1_10', 'dol_vehicle_id']\n",
      "Remaining cols: 15\n"
     ]
    }
   ],
   "source": [
    "# Summary: Drop ID-like columns and those with excessive missing values; keep the rest.\n",
    "missing_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "\n",
    "id_like = [c for c in df.columns if any(k in c for k in ['vin', 'vehicle_id', 'id'])]\n",
    "\n",
    "threshold = 0.8\n",
    "drop_cols = [c for c, r in missing_ratio.items() if r > threshold]\n",
    "keep_cols = set(df.columns) - set(id_like) - set(drop_cols)\n",
    "\n",
    "df = df[list(keep_cols)].copy()\n",
    "print('Dropped columns:', id_like + drop_cols)\n",
    "print('Remaining cols:', len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144a8ff",
   "metadata": {},
   "source": [
    "## 7) Text Cleaning\n",
    "Normalize spaces/case and unify common synonyms (e.g., BEV vs Battery Electric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9514954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Normalize text fields and harmonize EV type labels (BEV/PHEV).\n",
    "for c in df.select_dtypes(include='object').columns:\n",
    "    df[c] = df[c].astype(str).str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "if 'electric_vehicle_type' in df.columns:\n",
    "    df['electric_vehicle_type'] = df['electric_vehicle_type'].str.lower().replace({\n",
    "        'battery electric vehicle (bev)': 'bev',\n",
    "        'bev': 'bev',\n",
    "        'plug-in hybrid electric vehicle (phev)': 'phev',\n",
    "        'phev': 'phev'\n",
    "    })\n",
    "\n",
    "print('Text normalization complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0e4e8",
   "metadata": {},
   "source": [
    "## 8) Feature Transformation (dates, geospatial, numeric casts)\n",
    "Parse dates/years, extract lat/lon, coerce numeric types where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd8b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic type coercion done.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Coerce likely numeric fields and lat/lon columns to numeric types; parse year if present.\n",
    "import numpy as np\n",
    "\n",
    "# Example likely columns (adjust if present)\n",
    "possible_year_cols = [c for c in df.columns if 'model_year' in c or 'modelyear' in c]\n",
    "if possible_year_cols:\n",
    "    yr = possible_year_cols[0]\n",
    "    df[yr] = pd.to_numeric(df[yr], errors='coerce')\n",
    "\n",
    "for c in ['latitude', 'longitude', 'lat', 'lon']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Coerce numerics that look numeric but are object\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object:\n",
    "        sample = df[c].head(100).str.replace(',', '', regex=False)\n",
    "        if sample.str.match(r'^-?\\d+(\\.\\d+)?$').mean() > 0.7:\n",
    "            df[c] = pd.to_numeric(df[c].str.replace(',', '', regex=False), errors='ignore')\n",
    "\n",
    "print('Basic type coercion done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85791930",
   "metadata": {},
   "source": [
    "## 9) Outlier Detection and Treatment\n",
    "Use IQR on key numeric features and clip tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff666c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers clipped via IQR rule.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Clip numeric outliers using the IQR (1.5x) rule.\n",
    "def iqr_clip(series: pd.Series, k: float = 1.5):\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - k * iqr, q3 + k * iqr\n",
    "    return series.clip(lower=lo, upper=hi)\n",
    "\n",
    "for c in df.select_dtypes(include=[np.number]).columns:\n",
    "    df[c] = iqr_clip(df[c])\n",
    "\n",
    "print('Outliers clipped via IQR rule.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faadec7",
   "metadata": {},
   "source": [
    "## 10) Categorical Encoding and 11) Scaling\n",
    "Fit OneHotEncoder for categoricals and StandardScaler for numerics; persist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder/scaler persisted.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Fit OneHotEncoder for categoricals and StandardScaler for numerics; save both.\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "cat_cols = [c for c in df.select_dtypes(exclude=[np.number]).columns]\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns]\n",
    "\n",
    "# scikit-learn >= 1.2 uses sparse_output instead of sparse\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "if cat_cols:\n",
    "    enc.fit(df[cat_cols])\n",
    "    joblib.dump(enc, 'artifacts/encoder.pkl')\n",
    "if num_cols:\n",
    "    scaler.fit(df[num_cols])\n",
    "    joblib.dump(scaler, 'artifacts/scaler.pkl')\n",
    "\n",
    "print('Encoder/scaler persisted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6398d",
   "metadata": {},
   "source": [
    "## 12) Aggregation / Grouping (city/county level demand)\n",
    "Aggregate by city/county to compute EV counts and type splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by: ['city', 'county', 'state']\n",
      "Aggregated shape: (898, 6)\n",
      "Saved aggregated table.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Aggregate by area (city/county/state) to compute EV counts and type splits.\n",
    "group_keys = [k for k in ['city', 'county', 'state'] if k in df.columns]\n",
    "if not group_keys:\n",
    "    # fallback: census_tract or zip\n",
    "    for k in ['census_tract', 'postal_code', 'zip_code']:\n",
    "        if k in df.columns:\n",
    "            group_keys = [k]\n",
    "            break\n",
    "\n",
    "print('Grouping by:', group_keys)\n",
    "\n",
    "def is_type(val, key):\n",
    "    return 1 if str(val).lower() == key else 0\n",
    "\n",
    "agg_rows = []\n",
    "for key, g in df.groupby(group_keys):\n",
    "    row = {}\n",
    "    if isinstance(key, tuple):\n",
    "        for k, v in zip(group_keys, key):\n",
    "            row[k] = v\n",
    "    else:\n",
    "        row[group_keys[0]] = key\n",
    "    row['ev_count'] = len(g)\n",
    "    if 'electric_vehicle_type' in g.columns:\n",
    "        row['bev_count'] = g['electric_vehicle_type'].apply(lambda v: is_type(v, 'bev')).sum()\n",
    "        row['phev_count'] = g['electric_vehicle_type'].apply(lambda v: is_type(v, 'phev')).sum()\n",
    "    agg_rows.append(row)\n",
    "\n",
    "df_agg = pd.DataFrame(agg_rows)\n",
    "print('Aggregated shape:', df_agg.shape)\n",
    "\n",
    "df_agg.to_parquet('data/processed/agg_city_county.parquet', index=False)\n",
    "print('Saved aggregated table.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e98db5",
   "metadata": {},
   "source": [
    "## 13) Feature Engineering (density, ratios, geo buckets)\n",
    "Compute simple demand features and centroid lat/lon per area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e546a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete.\n",
      "Re-saved aggregated table with features.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Add centroid latitude/longitude and simple ratio features for each area.\n",
    "# Compute centroid lat/lon if available\n",
    "lat_col = next((c for c in ['latitude','lat'] if c in df.columns), None)\n",
    "lon_col = next((c for c in ['longitude','lon'] if c in df.columns), None)\n",
    "\n",
    "if lat_col and lon_col:\n",
    "    cent = df.groupby(group_keys)[[lat_col, lon_col]].mean().reset_index().rename(columns={lat_col:'centroid_lat', lon_col:'centroid_lon'})\n",
    "    df_agg = df_agg.merge(cent, on=group_keys, how='left')\n",
    "\n",
    "# Simple density proxies\n",
    "if {'bev_count','phev_count','ev_count'} <= set(df_agg.columns):\n",
    "    df_agg['bev_ratio'] = (df_agg['bev_count'] / df_agg['ev_count'].replace(0,1)).clip(0,1)\n",
    "    df_agg['phev_ratio'] = (df_agg['phev_count'] / df_agg['ev_count'].replace(0,1)).clip(0,1)\n",
    "\n",
    "print('Feature engineering complete.')\n",
    "\n",
    "df_agg.to_parquet('data/processed/agg_city_county.parquet', index=False)\n",
    "print('Re-saved aggregated table with features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048f326",
   "metadata": {},
   "source": [
    "## 14) Class/Target Definition\n",
    "Create regression target (required chargers) and classification target (high demand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets created: required_chargers, high_demand (threshold= 45.25 )\n"
     ]
    }
   ],
   "source": [
    "# Summary: Create targets—required chargers (regression) and high_demand flag (classification).\n",
    "import math\n",
    "\n",
    "vehicles_per_port = 20  # heuristic; adjust as needed\n",
    "df_agg['required_chargers'] = df_agg['ev_count'].apply(lambda v: int(math.ceil(v / max(1, vehicles_per_port))))\n",
    "\n",
    "# high demand if above 75th percentile\n",
    "th = df_agg['ev_count'].quantile(0.75)\n",
    "df_agg['high_demand'] = (df_agg['ev_count'] >= th).astype(int)\n",
    "\n",
    "print('Targets created: required_chargers, high_demand (threshold=', th, ')')\n",
    "\n",
    "df_agg.to_parquet('data/processed/agg_targets.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa1f4f",
   "metadata": {},
   "source": [
    "## 15) Train/Validation Split\n",
    "Split with stratification on high_demand to stabilize classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val shapes: (718, 5) (180, 5)\n"
     ]
    }
   ],
   "source": [
    "# Summary: Split data into train/validation sets (stratified by high_demand) using numeric features.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = [c for c in df_agg.columns if c not in set(group_keys + ['required_chargers','high_demand'])]\n",
    "X = df_agg[feature_cols].copy()\n",
    "y_reg = df_agg['required_chargers']\n",
    "y_cls = df_agg['high_demand']\n",
    "\n",
    "# Simple numeric-only subset for baseline models; one-hot could be added later\n",
    "X_num = X.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "X_train, X_val, y_reg_train, y_reg_val, y_cls_train, y_cls_val = train_test_split(\n",
    "    X_num, y_reg, y_cls, test_size=0.2, random_state=42, stratify=y_cls\n",
    ")\n",
    "\n",
    "print('Train/val shapes:', X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e300fcd",
   "metadata": {},
   "source": [
    "## 16) Baseline Models (Linear/Logistic)\n",
    "Train and evaluate simple baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_rmse': np.float64(0.2694131748942958), 'reg_mae': 0.217652930957749, 'cls_f1': 1.0, 'cls_auc': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Summary: Train/evaluate baseline linear (regression) and logistic (classification) models.\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "lin = LinearRegression().fit(X_train, y_reg_train)\n",
    "reg_pred = lin.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_val, reg_pred))\n",
    "mae = mean_absolute_error(y_reg_val, reg_pred)\n",
    "\n",
    "log = LogisticRegression(max_iter=1000).fit(X_train, y_cls_train)\n",
    "cls_proba = log.predict_proba(X_val)[:,1]\n",
    "cls_pred = (cls_proba >= 0.5).astype(int)\n",
    "f1 = f1_score(y_cls_val, cls_pred)\n",
    "try:\n",
    "    auc = roc_auc_score(y_cls_val, cls_proba)\n",
    "except Exception:\n",
    "    auc = float('nan')\n",
    "\n",
    "print({'reg_rmse': rmse, 'reg_mae': mae, 'cls_f1': f1, 'cls_auc': auc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78431ea6",
   "metadata": {},
   "source": [
    "## 17) Tree-Based Models\n",
    "RandomForest and Gradient Boosting (XGBoost/LightGBM) for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fe6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf_reg_rmse': np.float64(2.9014265222813287), 'rf_reg_mae': 0.42442592592592576, 'rf_cls_f1': 1.0, 'rf_cls_auc': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Summary: Train/evaluate Random Forest models for regression and classification.\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=300, random_state=42).fit(X_train, y_reg_train)\n",
    "rf_reg_pred = rf_reg.predict(X_val)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_reg_val, rf_reg_pred))\n",
    "rf_mae = mean_absolute_error(y_reg_val, rf_reg_pred)\n",
    "\n",
    "rf_cls = RandomForestClassifier(n_estimators=300, random_state=42).fit(X_train, y_cls_train)\n",
    "rf_cls_proba = rf_cls.predict_proba(X_val)[:,1]\n",
    "rf_cls_pred = (rf_cls_proba >= 0.5).astype(int)\n",
    "rf_f1 = f1_score(y_cls_val, rf_cls_pred)\n",
    "rf_auc = roc_auc_score(y_cls_val, rf_cls_proba)\n",
    "\n",
    "print({'rf_reg_rmse': rf_rmse, 'rf_reg_mae': rf_mae, 'rf_cls_f1': rf_f1, 'rf_cls_auc': rf_auc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10941e",
   "metadata": {},
   "source": [
    "## 18) Hyperparameter Tuning (RandomizedSearch)\n",
    "Quick randomized search on RF as a demo (can expand as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b98126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF reg params: {'n_estimators': 300, 'min_samples_split': 5, 'max_depth': 20}\n",
      "Best RF cls params: {'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "# Summary: Run RandomizedSearchCV to tune Random Forest hyperparameters for both tasks.\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [None, 8, 12, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "rf_reg_cv = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_grid, n_iter=6, cv=3, n_jobs=-1, random_state=42)\n",
    "rf_reg_cv.fit(X_train, y_reg_train)\n",
    "print('Best RF reg params:', rf_reg_cv.best_params_)\n",
    "\n",
    "rf_cls_cv = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, n_iter=6, cv=3, n_jobs=-1, random_state=42)\n",
    "rf_cls_cv.fit(X_train, y_cls_train)\n",
    "print('Best RF cls params:', rf_cls_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4db9c6",
   "metadata": {},
   "source": [
    "## 19) Model Evaluation and Selection + 20) Persist Artifacts\n",
    "Pick the better model(s) and save them with feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a31fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved selected models and feature list.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Select the best models (CV if available) and persist them plus the feature list.\n",
    "import json, joblib, os\n",
    "\n",
    "best_reg = rf_reg_cv.best_estimator_ if 'rf_reg_cv' in globals() else rf_reg\n",
    "best_cls = rf_cls_cv.best_estimator_ if 'rf_cls_cv' in globals() else rf_cls\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(best_reg, 'models/best_regressor.joblib')\n",
    "joblib.dump(best_cls, 'models/best_classifier.joblib')\n",
    "\n",
    "with open('artifacts/feature_list.json','w') as f:\n",
    "    json.dump({'features': feature_cols, 'numeric_only': list(X_num.columns)}, f, indent=2)\n",
    "\n",
    "print('Saved selected models and feature list.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3036fd",
   "metadata": {},
   "source": [
    "## 21) Optimal Station Placement via Clustering (KMeans)\n",
    "Cluster high-demand centroids to get k candidate station locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860f3b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['centroid_lat', 'centroid_lon']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_30376\\776004637.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m sklearn.cluster \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[32m      2\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m hd = df_agg[df_agg[\u001b[33m'high_demand'\u001b[39m]==\u001b[32m1\u001b[39m].dropna(subset=[\u001b[33m'centroid_lat'\u001b[39m,\u001b[33m'centroid_lon'\u001b[39m])\n\u001b[32m      4\u001b[39m coords = hd[[\u001b[33m'centroid_lat'\u001b[39m,\u001b[33m'centroid_lon'\u001b[39m]].to_numpy()\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m k = min(\u001b[32m20\u001b[39m, max(\u001b[32m2\u001b[39m, len(hd)//\u001b[32m10\u001b[39m))  \u001b[38;5;66;03m# heuristic\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\comag\\Desktop\\EV\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6688\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6689\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6690\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6691\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6692\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6693\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6694\u001b[39m \n\u001b[32m   6695\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['centroid_lat', 'centroid_lon']"
     ]
    }
   ],
   "source": [
    "# Summary: Cluster high-demand areas to suggest candidate station locations via KMeans.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "hd = df_agg[df_agg['high_demand']==1].dropna(subset=['centroid_lat','centroid_lon'])\n",
    "coords = hd[['centroid_lat','centroid_lon']].to_numpy()\n",
    "\n",
    "k = min(20, max(2, len(hd)//10))  # heuristic\n",
    "km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "labels = km.fit_predict(coords)\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "stations = pd.DataFrame({\n",
    "    'station_id': [f'kmeans_{i}' for i in range(len(centroids))],\n",
    "    'latitude': centroids[:,0],\n",
    "    'longitude': centroids[:,1],\n",
    "    'method': 'kmeans'\n",
    "})\n",
    "\n",
    "stations.to_csv('artifacts/stations.csv', index=False)\n",
    "print('Saved KMeans station candidates:', stations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2185895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_agg centroid cols present: ['centroid_lat', 'centroid_lon']\n",
      "Non-null centroid rows: 898\n"
     ]
    }
   ],
   "source": [
    "# Summary: If centroid_lat/lon are missing, derive them from available lat/lon or parse location strings.\n",
    "# Fallback: derive centroid_lat/lon if missing using available columns\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "if not set(['centroid_lat','centroid_lon']).issubset(df_agg.columns):\n",
    "    # Try to find lat/lon columns in df\n",
    "    lat_candidates = [c for c in df.columns if re.search(r'lat', c, re.I)]\n",
    "    lon_candidates = [c for c in df.columns if re.search(r'lon|lng|long', c, re.I)]\n",
    "    lat2 = next((c for c in lat_candidates if df[c].notna().any()), None)\n",
    "    lon2 = next((c for c in lon_candidates if df[c].notna().any()), None)\n",
    "\n",
    "    merged = False\n",
    "    if lat2 and lon2:\n",
    "        cent = (\n",
    "            df.groupby(group_keys)[[lat2, lon2]]\n",
    "              .mean()\n",
    "              .reset_index()\n",
    "              .rename(columns={lat2:'centroid_lat', lon2:'centroid_lon'})\n",
    "        )\n",
    "        df_agg = df_agg.merge(cent, on=group_keys, how='left')\n",
    "        merged = True\n",
    "\n",
    "    if not merged:\n",
    "        # Try to parse a location field like 'Vehicle Location' or 'Location'\n",
    "        loc_col = next((c for c in df.columns if 'location' in c.lower()), None)\n",
    "        if loc_col is not None:\n",
    "            def extract_lat_lon(s):\n",
    "                if pd.isna(s):\n",
    "                    return pd.Series([None, None])\n",
    "                s = str(s)\n",
    "                # Format: POINT (lon lat)\n",
    "                m = re.search(r'POINT\\s*\\(\\s*(-?\\d+\\.\\d+)\\s+(-?\\d+\\.\\d+)\\s*\\)', s)\n",
    "                if m:\n",
    "                    lon = float(m.group(1)); lat = float(m.group(2))\n",
    "                    return pd.Series([lat, lon])\n",
    "                # Format: (lat, lon)\n",
    "                m = re.search(r'\\(\\s*(-?\\d+\\.\\d+)\\s*,\\s*(-?\\d+\\.\\d+)\\s*\\)', s)\n",
    "                if m:\n",
    "                    lat = float(m.group(1)); lon = float(m.group(2))\n",
    "                    return pd.Series([lat, lon])\n",
    "                return pd.Series([None, None])\n",
    "\n",
    "            tmp = df[group_keys + [loc_col]].copy()\n",
    "            tmp[['__lat','__lon']] = tmp[loc_col].apply(extract_lat_lon)\n",
    "            cent = (\n",
    "                tmp.dropna(subset=['__lat','__lon'])\n",
    "                   .groupby(group_keys)[['__lat','__lon']]\n",
    "                   .mean()\n",
    "                   .reset_index()\n",
    "                   .rename(columns={'__lat':'centroid_lat','__lon':'centroid_lon'})\n",
    "            )\n",
    "            df_agg = df_agg.merge(cent, on=group_keys, how='left')\n",
    "\n",
    "print('df_agg centroid cols present:', [c for c in df_agg.columns if c in ['centroid_lat','centroid_lon']])\n",
    "print('Non-null centroid rows:', df_agg[['centroid_lat','centroid_lon']].dropna().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411faa1",
   "metadata": {},
   "source": [
    "## 22) Optimal Station Placement via Facility Location (ILP with PuLP)\n",
    "Optional: p-median style optimization from demand centroids to candidate stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILP chose 10 sites. Updated artifacts/stations.csv\n"
     ]
    }
   ],
   "source": [
    "# Summary: Solve a p-median facility location problem to pick p best station sites among candidates.\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "from itertools import product\n",
    "import pulp as pl\n",
    "\n",
    "# Distance helper (haversine km)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Use KMeans centroids as candidate sites\n",
    "cand = stations[['station_id','latitude','longitude']].copy()\n",
    "demand = hd[['centroid_lat','centroid_lon']].copy().reset_index(drop=True)\n",
    "\n",
    "p = min(10, len(cand))  # number of stations to open\n",
    "\n",
    "# Build distance matrix\n",
    "D = {}\n",
    "for i in range(len(demand)):\n",
    "    for j in range(len(cand)):\n",
    "        D[(i,j)] = haversine_km(demand.loc[i,'centroid_lat'], demand.loc[i,'centroid_lon'], cand.loc[j,'latitude'], cand.loc[j,'longitude'])\n",
    "\n",
    "prob = pl.LpProblem('p_median', pl.LpMinimize)\n",
    "open_j = pl.LpVariable.dicts('open', list(range(len(cand))), 0, 1, pl.LpBinary)\n",
    "assign_ij = pl.LpVariable.dicts('assign', (list(range(len(demand))), list(range(len(cand)))), 0, 1, pl.LpBinary)\n",
    "\n",
    "# Objective: minimize distance\n",
    "prob += pl.lpSum(D[(i,j)] * assign_ij[i][j] for i in range(len(demand)) for j in range(len(cand)))\n",
    "\n",
    "# Each demand assigned to one open site\n",
    "for i in range(len(demand)):\n",
    "    prob += pl.lpSum(assign_ij[i][j] for j in range(len(cand))) == 1\n",
    "    for j in range(len(cand)):\n",
    "        prob += assign_ij[i][j] <= open_j[j]\n",
    "\n",
    "# Exactly p sites open\n",
    "prob += pl.lpSum(open_j[j] for j in range(len(cand))) == p\n",
    "\n",
    "_ = prob.solve(pl.PULP_CBC_CMD(msg=False))\n",
    "chosen = [j for j in range(len(cand)) if open_j[j].value() == 1]\n",
    "\n",
    "ilp_sites = cand.iloc[chosen].copy()\n",
    "ilp_sites['method'] = 'ilp_p_median'\n",
    "\n",
    "all_sites = pd.concat([stations, ilp_sites], ignore_index=True)\n",
    "all_sites.to_csv('artifacts/stations.csv', index=False)\n",
    "print('ILP chose', len(ilp_sites), 'sites. Updated artifacts/stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ad085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifacts/stations_map.html\n"
     ]
    }
   ],
   "source": [
    "# Summary: Plot suggested station sites on a Folium map and save to artifacts.\n",
    "import folium\n",
    "\n",
    "if not stations_df.empty:\n",
    "    m = folium.Map(location=[stations_df['latitude'].mean(), stations_df['longitude'].mean()], zoom_start=6)\n",
    "    for _, r in stations_df.iterrows():\n",
    "        folium.Marker([r['latitude'], r['longitude']], popup=r['station_id']).add_to(m)\n",
    "    m.save('artifacts/stations_map.html')\n",
    "    print('Saved artifacts/stations_map.html')\n",
    "else:\n",
    "    print('No stations to plot yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54f9b8",
   "metadata": {},
   "source": [
    "## 26) Export GeoJSON/CSV for App\n",
    "Export artifacts for the Streamlit app to load quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported artifacts for app.\n"
     ]
    }
   ],
   "source": [
    "# Summary: Export aggregated demand and station recommendations as CSVs for the Streamlit app.\n",
    "df_agg.to_csv('artifacts/aggregated_demand.csv', index=False)\n",
    "stations_df.to_csv('artifacts/stations.csv', index=False)\n",
    "print('Exported artifacts for app.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c5fe6",
   "metadata": {},
   "source": [
    "## 27) Streamlit App: Nearest Charger and Planner Pages\n",
    "We created `app/app.py`. After running this notebook to generate artifacts, launch the app to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4495c3",
   "metadata": {},
   "source": [
    "## 28) Packaging and Run Scripts (CLI + Streamlit)\n",
    "We added `src/cli.py` to guide steps. To run the app after artifacts generation, use a terminal:\n",
    "- streamlit run app/app.py\n",
    "\n",
    "## 29) Unit Tests for Core Utils\n",
    "You can add pytest tests under `tests/` as needed (omitted here for brevity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
